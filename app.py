# -*- coding: utf-8 -*-
"""SYNTHESE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DeRgXDm2Vwafea-0ytm-bJ6JUw3b5gkh

# Impl√©mentation d'un Chatbot documentaire (RAG)

# Pr√©requis : Installation des librairies n√©cessaires
"""
from llama_cpp import Llama

model_path = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"  # nom du mod√®le sur HF Hub
llm = Llama(model_path=model_path)

# Pour importer les fichiers PDF
import os
import requests

# Extraction des fichiers PDF
import PyPDF2

# Traitement du texte
import re
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Mod√®le TF
from sklearn.feature_extraction.text import CountVectorizer

# Mod√®le Dense Embeding
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim
from tqdm import tqdm

# Mod√®le Reranker
from FlagEmbedding import FlagReranker

# Trac√© des graphes de r√©sultat
import matplotlib.pyplot as plt

# Objet text retriever

import langchain as lc
from langchain_chroma import Chroma
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from FlagEmbedding import FlagReranker

# Mod√®le LLM


# Hallucinations
from langchain.llms import LlamaCpp
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from llama_cpp import Llama
from math import *
import numpy as np
from transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer
import torch
from tqdm import tqdm
nltk.download('punkt')
# T√©l√©chargement du mod√®le pour hallucinations
import pandas as pd
import torch
import evaluate
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from detoxify import Detoxify
from sklearn.metrics import precision_recall_curve, auc

# -*- coding: utf-8 -*-
"""SYNTHESE.ipynb - Version corrig√©e

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DeRgXDm2Vwafea-0ytm-bJ6JUw3b5gkh

# Impl√©mentation d'un Chatbot documentaire (RAG)

# Pr√©requis : Installation des librairies n√©cessaires
"""

# CORRECTION KERAS - √Ä EX√âCUTER EN PREMIER
import os
os.environ['TF_USE_LEGACY_KERAS'] = '1'

# Pour importer les fichiers PDF
import requests

# Extraction des fichiers PDF
# !pip install PyPDF2
import PyPDF2

# Traitement du texte
# !pip install nltk
# !pip install re
import re
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
# !pip install -qU langchain-text-splitters
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Mod√®le TF
from sklearn.feature_extraction.text import CountVectorizer
# !pip install sentence-transformers

# Mod√®le Dense Embeding
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim
from tqdm import tqdm

# Mod√®le Reranker
# !pip install FlagEmbedding
from FlagEmbedding import FlagReranker

# Trac√© des graphes de r√©sultat
import matplotlib.pyplot as plt

# Objet text retriever
# !pip install langchain
# !pip install gradio
import langchain as lc
# !pip install langchain --q
# !pip install langchain-community --q
# !pip install langchain-chroma --q
# !pip install FlagEmbedding -q
from langchain_chroma import Chroma
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from FlagEmbedding import FlagReranker

# Mod√®le LLM
# !pip install transformers
# !pip install huggingface_hub
# !pip install llama-cpp-python

# Hallucinations
from langchain.llms import LlamaCpp
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory  # CORRECTION: Import manquant
from llama_cpp import Llama
from math import *
import numpy as np
from transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer
import torch
from tqdm import tqdm

# T√©l√©chargement du mod√®le pour hallucinations
# !huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False

# Analyse de toxicit√©
# !pip install torch
# !pip install detoxify
# !pip install datasets
# !pip install scikit-learn
# !pip install evaluate
# !pip install pandas

import pandas as pd
import torch
import evaluate
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from detoxify import Detoxify
from sklearn.metrics import precision_recall_curve, auc

"""# GESTION DE LA BASE DE DONN√âES

## Etape 1 : r√©cup√©ration des fichiers PDFs:
"""

# Chemin du dossier o√π l'on souhaite t√©l√©charger les fichiers
chemin_dossier = "/Users/macbookair/Desktop/RAG/RAG_IPCC"

# V√©rifier si le dossier existe, sinon le cr√©er
if not os.path.exists(chemin_dossier):
    os.makedirs(chemin_dossier)
    print("Le dossier 'RAG_IPCC' a √©t√© cr√©√© avec succ√®s.")
else:
    print("Le dossier 'RAG_IPCC' existe d√©j√†.")

# URLs des fichiers √† t√©l√©charger
urls = {
    "6th_report": "https://www.ipcc.ch/report/ar6/syr/downloads/report/IPCC_AR6_SYR_FullVolume.pdf",
    "ocean": "https://www.ipcc.ch/site/assets/uploads/sites/3/2022/03/02_SROCC_TS_FINAL.pdf",
    "land": "https://www.ipcc.ch/site/assets/uploads/sites/4/2022/11/SRCCL_Technical-Summary.pdf",
    "warming": "https://www.ipcc.ch/site/assets/uploads/sites/2/2022/06/SPM_version_report_LR.pdf"
}

# T√©l√©charger les fichiers dans le dossier
for name, url in urls.items():
    response = requests.get(url)
    with open(os.path.join(chemin_dossier, f"{name}.pdf"), 'wb') as file:
        file.write(response.content)
    print(f"{name} a √©t√© t√©l√©charg√©.")

"""## Etape 2 : Extraction du texte des fichiers PDF"""

# Chemin du dossier contenant les fichiers PDF
chemin_dossier = "/Users/macbookair/Desktop/RAG/RAG_IPCC"

# Liste des fichiers PDF dans le dossier
fichiers_pdf = [f for f in os.listdir(chemin_dossier) if f.endswith('.pdf')]

# Liste pour stocker le texte extrait de chaque PDF
extracted_text = []

# Boucle √† travers chaque fichier PDF
for pdf in fichiers_pdf:
    print(f"*** PROCESSING FILE : {pdf} ***")

    # Chemin complet du fichier PDF
    chemin_pdf = os.path.join(chemin_dossier, pdf)

    # Ouverture du fichier PDF en mode lecture binaire
    with open(chemin_pdf, 'rb') as file:
        # Cr√©ation d'un objet de lecteur PDF
        pdf_reader = PyPDF2.PdfReader(file)

        # Boucle √† travers chaque page du PDF
        for page_num in range(len(pdf_reader.pages)):
            # Extraction du texte de la page actuelle
            page = pdf_reader.pages[page_num]
            text = page.extract_text()

            # Ajout du texte extrait √† la liste
            extracted_text.append({"document": pdf, "page": page_num, "content": text})

# Affichage du texte extrait
for text in extracted_text:
    print(text)

"""## Etape 3 : Traitement du texte en chunks propres"""

#### FONCTIONS ####

# Segmentation du texte de base

def splitting_by_numer_of_words(text, chunk_size):
  """
  D√©coupe un texte en chunks de taille donn√©e (nombre de caract√®res).

  Args:
    text (str): Le texte √† splitter.
    chunk_size (int): La taille souhait√©e des chunks (nombre de mots).

  Returns:
    list: Une liste de chunks de texte.
  """
  chunks = []
  for phrase in text.split('\n'):
    words = phrase.split()
    for i in range(0, len(words), chunk_size):
      chunks.append(' '.join(words[i:i + chunk_size]))
  return chunks

# Fonction de splitting par phrase

def splitting_by_sentences(text):
  """
  D√©coupe un texte en chunks par phrases.

  Args:
    text (str): Le texte √† d√©couper.

  Returns:
    list: Une liste de chunks de texte (phrases).
  """
  sentences = []
  list_paragraph = text.split("\n")
  for paragraph in list_paragraph:
    list_sent = paragraph.split(".")
    sentences = sentences + list_sent
  return sentences

## TEST
print(splitting_by_numer_of_words("Bonjour, aujourd'hui c'est. le 26 Mars 2019 √ßa marche?",5))

# Fonction de splitting par phrase
def splitting_by_sentences(text):
  sentences=text.split('.')
  return sentences

## TEST
print(splitting_by_sentences("Bonjour, aujourd'hui c'est. le 26 Mars 2019 √ßa marche?"))

# Nettoyage du contenu de chaque chunk

special_chars = [" ", '-', '&', '(', ')', '_', ';', '‚Ä†', '+', '‚Äì', "'", '!', '[', ']', "'", 'ÃÅ', 'ÃÄ', '\u2009', '\u200b', '\u202f', '¬©', '¬£', '¬ß', '¬∞', '@', '‚Ç¨', '$', '\xa0', '~','\n','ÔøΩ']

def remove_char(text, char):
    """Remove each specific character from the text for each character in the chars list."""
    return text.replace(char, ' ')

def remove_chars(text, chars):
    """ Apply remove_char() function to text """
    for char in chars:
        text = remove_char(text, char)
    return text

def remove_multiple_white_spaces(text):
    """Remove multiple spaces."""
    text = re.sub(" +", " ", text)
    return text

def clean_text(text, special_chars=special_chars):
    """Generate a text without chars expect points and comma and multiple white spaces."""
    text = remove_chars(text, special_chars)
    text = remove_multiple_white_spaces(text)
    return text

# Filtrage des mots vides

def contains_mainly_digits(text, threshold=0.5):
    """
    Checks if a text string contains a high percentage of digits compared to letters.

    Args:
        text (str): The input text to analyze.
        threshold (float, optional): The threshold value for the proportion of digits to letters.
            Defaults to 0.5.

    Returns:
        bool: True if the proportion of digits in the text exceeds the threshold, False otherwise.
    """
    if not text:
        return False
    letters_count = 0
    nbs_count = 0
    for char in text:
        if char.isalpha():
            letters_count += 1
        elif char.isdigit():
            nbs_count += 1
    if letters_count + nbs_count > 0:
        digits_pct = (nbs_count / (letters_count + nbs_count))
    else:
        return True
    return digits_pct > threshold

def remove_mostly_digits_chunks(chunks, threshold=0.5):
  return [chunk for chunk in chunks if not contains_mainly_digits(chunk['content'])]

#### EXECUTION ####

# Split intelligent avec diff√©rents param√®tres
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=20,
    length_function=len,
    is_separator_regex=False,
)

# Split pertinent qui garde la structure du document
chunks = []
for page_content in extracted_text:
  chunks_list = text_splitter.split_text(page_content['content'])

  # chunks_list = splitting_by_numer_of_words(page_content['content'])
  # chunks_list = splitting_by_sentences(page_content['content'])
  for chunk in chunks_list:
    text=clean_text(chunk)
    chunks.append({"document": page_content['document'],
                   "page": page_content['page'],
                   "content": text})
chunks=remove_mostly_digits_chunks(chunks)
print(chunks)

"""# COMPARAISON DES MODELES DE RECHERCHE (Information Retrieval)"""

"""## Etape 1 : Impl√©mentation du mod√®le BOW (TF-IDF)"""

"""## Etape 2 : Impl√©mentation du mod√®le Dense Embeding"""

"""# IMPLEMENTATION DU MODELE DE RECHERCHE RETENU"""

class TextRetriever:
    def __init__(self, embedding_model_name="mixedbread-ai/mxbai-embed-large-v1", reranking_model_name="BAAI/bge-reranker-large"):
        """
        Initialise les mod√®les d'embedding et de reranking.

        Args:
            embedding_model_name (str): Nom du mod√®le d'embedding.
            reranking_model_name (str): Nom du mod√®le de reranking.
        """
        self.embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)
        self.reranker_model = FlagReranker(reranking_model_name, use_fp16=True)
        self.vector_database = None  # Initialisation de la base de donn√©es vectorielle √† None

    def store_embeddings(self, chunks, path="./chroma_db"):
        """
        Stocke les embeddings des chunks de texte dans une base de donn√©es vectorielle.

        Args:
            chunks (list of str): Liste de chunks de texte √† stocker.
            path (str): Chemin du r√©pertoire o√π la base de donn√©es sera stock√©e.
        """
        self.vector_database = Chroma.from_texts(chunks, embedding=self.embedding_model, persist_directory=path)

    def load_embeddings(self, path):
        """
        Charge les embeddings depuis une base de donn√©es vectorielle.

        Args:
            path (str): Chemin du r√©pertoire de la base de donn√©es.
        """
        self.vector_database = Chroma(persist_directory=path, embedding=self.embedding_model)

    def get_best_chunks(self, query, top_k=3):
        """
        Recherche les meilleurs chunks correspondant √† une requ√™te.

        Args:
            query (str): Requ√™te de recherche.
            top_k (int): Nombre de meilleurs chunks √† retourner.

        Returns:
            list: Liste des meilleurs chunks correspondant √† la requ√™te.
        """
        best_chunks = self.vector_database.similarity_search(query, k=top_k)
        return best_chunks

    def rerank_chunks(self, query, chunks):
        """
        Retourne le chunk le plus pertinent pour une requ√™te donn√©e.

        Args:
            query (str): Requ√™te de recherche.
            chunks (list): Liste des chunks √† re-classer.

        Returns:
            list: Liste des chunks tri√©s par pertinence.
        """
        best_chunks = self.get_best_chunks(query, top_k=10)
        rerank_scores = []
        # CORRECTION: Utiliser chunk_texts au lieu de text_chunks
        chunk_texts = [chunk.page_content if hasattr(chunk, 'page_content') else str(chunk) for chunk in best_chunks]
        for text in chunk_texts:
          score = self.reranker_model.compute_score([query, text])
          rerank_scores.append(score)

        return [x for _, x in sorted(zip(rerank_scores, best_chunks), reverse=True)]

    def get_context(self, query):
        """
        Retourne le chunk le plus pertinent pour une requ√™te donn√©e.

        Args:
            query (str): Requ√™te de recherche.

        Returns:
            str: Contenu du chunk le plus pertinent.
        """
        best_chunks = self.get_best_chunks(query, top_k=1)
        return best_chunks[0].page_content

retriever=TextRetriever()

all_chunks=[]
for chunk in chunks:
  all_chunks.append(chunk['content'])
retriever.store_embeddings(all_chunks)

"""# MODELE LLM

## Etape 1 : Generation d'une r√©ponse
"""

def load_llm(model_path):
    # On charge le LLM sous format quantis√©. Cf la descriptions des param√®tres ci-dessous.
    llm = LlamaCpp(
        model_path=model_path, stop=["Question"], max_tokens=300, temperature=0,
        n_ctx=8000, n_batch=1024, n_gpu_layers=-1, logits_all=True
    )
    return llm

llm = load_llm("mistral-7b-instruct-v0.2.Q4_K_M.gguf")

## FONCTIONS

# Basic context function.
def get_context_from_query(query):
  context1=retriever.get_best_chunks(query,4)
  return context1

# Fonction de generation de texte
def conv_chain(llm):
    template = """Below is an instruction that describes a task. Write a response that appropriately completes the request using the context provided.

    Human: [INST] {instruction} [\INST]

    Context: {context}

    AI:\n
    """

    prompt = PromptTemplate(
        input_variables=["instruction",'context'], template=template
    )

    llm_chain = LLMChain(
        llm=llm,
        prompt=prompt,
        verbose=True,
    )

    return llm_chain

# N√©cessit√© d'ajouter un historique pour que la conversation ait un sens

"""## Etape 2 : Sauvegarde d'un historique limit√© de conversation"""

class ConversationHistoryLoader:

  def __init__(self, k):
    self.k=k
    self.conversation_history = []

  # Fonction qui permet cr√©er un prompt (string) sur l'historique de conversation.
  def create_conversation_history_prompt(self):
    conversation = ''

    if self.conversation_history == None:
      return conversation
    else:
      for exchange in reversed(self.conversation_history):
        conversation = conversation + '\nHuman: '+exchange['Human']+'\nAI: '+exchange['AI']
      return conversation

  # Fonction qui permet de mettre √† jour l'historique de conversation
  # √† partir de la derni√®re query et la derni√®re r√©ponse du LLM.
  def update_conversation_history(self, query, response):
    exchange = {'Human': query, 'AI': response}
    self.conversation_history.insert(0, exchange)

    if len(self.conversation_history) > self.k:
      self.conversation_history.pop()

def conv_chain_with_history(llm):
    template = """Below is an instruction that describes a task. Write a response that appropriately completes the request using the context provided and the previous conversation.

    Context: {context}

    {chat_history}

    Human: [INST] {instruction} [\INST]

    AI:\n
    """

    prompt = PromptTemplate(input_variables=["instruction",
                                             'chat_history',
                                             'context'],
                            template=template)

    llm_chain = LLMChain(
        llm=llm,
        prompt=prompt,
        verbose=True,
    )

    return llm_chain

def conv_chain_with_conversation_buffer(llm):
    template = """Below is an instruction that describes a task. Write a response that appropriately completes the request using the context provided.

    {chat_history}

    Human: [INST] {instruction} [\INST]

    Context: {context}

    AI:\n
    """

    prompt = PromptTemplate(
        input_variables=["instruction",'chat_history', 'context'], template=template
    )

    memory = ConversationBufferWindowMemory(memory_key="chat_history", input_key="instruction", k=3)

    llm_chain = LLMChain(
        llm=llm,
        prompt=prompt,
        verbose=True,
        memory=memory
    )

    return llm_chain

chain_with_history = conv_chain_with_history(llm)
def get_context_from_query(query):
  context1=retriever.get_best_chunks(query,4)


  return context1

# Interface Gradio corrig√©e pour le RAG Chatbot

import gradio as gr

# CORRECTION 1: Cr√©er l'instance de ConversationHistoryLoader
ch = ConversationHistoryLoader(k=3)  # Garde les 3 derniers √©changes

def get_context_from_query(query):
    chunks = retriever.get_best_chunks(query, 4)
    # CORRECTION PRINCIPALE: Extraire le texte des chunks au lieu de retourner les objets
    context_parts = []
    for chunk in chunks:
        if hasattr(chunk, 'page_content'):
            context_parts.append(chunk.page_content)
        else:
            context_parts.append(str(chunk))
    return "\n\n".join(context_parts)  # Retourner du texte, pas des objets

def get_response(query):
    try:
        # Obtenir le contexte (maintenant sous forme de texte)
        context = get_context_from_query(query)
        
        # CORRECTION 2: Utiliser 'run' au lieu de 'predict'
        res = chain_with_history.run(
            instruction=query,
            context=context,
            chat_history=ch.create_conversation_history_prompt()
        )
        
        # CORRECTION 3: Mettre √† jour l'historique
        ch.update_conversation_history(query, res)
        
        # CORRECTION 4: Nettoyer la r√©ponse des artefacts de debug
        import re
        # Supprimer les patterns de debug qui apparaissent
        res = re.sub(r'\[context\..*?\]', '', res)
        res = re.sub(r'Al:\s*', '', res)
        res = res.strip()
        
        return res
        
    except Exception as e:
        return f"Erreur: {str(e)}"

# Interface Gradio (supprimer la duplication)
iface = gr.Interface(
    fn=get_response, 
    inputs=gr.Textbox(lines=2, placeholder="Posez votre question sur le climat..."), 
    outputs=gr.Textbox(lines=5, label="R√©ponse"),
    title="üåç RAG Chatbot - Questions Climatiques", 
    description="Posez vos questions sur le changement climatique bas√©es sur les rapports IPCC.",
    examples=[
        "Quels sont les principaux impacts du r√©chauffement climatique ?",
        "Comment les oc√©ans sont-ils affect√©s par le changement climatique ?",
        "Quelles sont les solutions pour r√©duire les √©missions ?"
    ]
)

# CORRECTION 5: Une seule ligne de lancement
iface.launch(share=True)