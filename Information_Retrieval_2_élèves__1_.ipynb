{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Information Retrieval 2/2"
      ],
      "metadata": {
        "id": "IAuoFHU30qeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette partie, l'objectif est de mettre en application les meilleurs méthodes d'Information Retrieval pour votre outil de RAG.\n",
        "\n",
        "\n",
        "\n",
        "Pour cela vous devrez :\n",
        "\n",
        "\n",
        "1.   Déterminer les modèles les plus appropriés (s'appuyer sur la partie précédente)\n",
        "2.   Pour les modèles d'embeddings, déterminer une base de donnée vectorielle appropriée\n",
        "3.   Implémenter la classe si dessous.\n",
        "\n",
        "Conseils :\n",
        "* Le framework *langchain* permet de réaliser ces tâches assez simplement.\n",
        "* Vous pouvez tester les méthodes sur les passages test de la partie précédente et sur les documents de la première séance.\n",
        "* Testez plusieurs méthodes (différents paramètres, renvoie d'un ou plusieurs passages concaténés...)"
      ],
      "metadata": {
        "id": "vy9VyS8IURLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classe à implémenter\n",
        "# Les méthodes et signatures sont indicatives et peuvent être modifiées\n",
        "\n",
        "class TextRetriever:\n",
        "    def __init__(self, ...):\n",
        "        \"\"\"\n",
        "        Initialise les modèles et la base de données vectorielle.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "    def store_embeddings(self, chunks, path=...):\n",
        "        \"\"\"\n",
        "        Stocke les embeddings des chunks de texte dans une base de données vectorielle.\n",
        "\n",
        "        Args:\n",
        "            chunks (list of str): Liste de chunks de texte à stocker.\n",
        "            path (str): Chemin du répertoire où la base de données sera stockée.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "    def load_embeddings(self, path):\n",
        "        \"\"\"\n",
        "        Charge les embeddings depuis une base de données vectorielle.\n",
        "\n",
        "        Args:\n",
        "            path (str): Chemin du répertoire de la base de données.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "    def get_best_chunks(self, query):\n",
        "        \"\"\"\n",
        "        Recherche les meilleurs chunks correspondant à une requête.\n",
        "\n",
        "        Args:\n",
        "            query (str): Requête de recherche.\n",
        "            top_k (int): Nombre de meilleurs chunks à retourner.\n",
        "\n",
        "        Returns:\n",
        "            list: Liste des meilleurs chunks correspondant à la requête.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def rerank_chunks(self, query, chunks):\n",
        "        \"\"\"\n",
        "        Retrie les chunks par pertinence\n",
        "        Args:\n",
        "            query (str): Requête de recherche.\n",
        "\n",
        "        Returns:\n",
        "            list: liste triée des chunks par pertinence\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def get_context(self, query):\n",
        "        \"\"\"\n",
        "        Retourne un texte contenant les informations pertinentes pour la requête.\n",
        "\n",
        "        Args:\n",
        "            query (str): Requête de recherche.\n",
        "\n",
        "        Returns:\n",
        "            str: texte pertinent pour répondre\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "uVSyXkiUNnVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "*   Décrivez et motivez la méthode choisie (modèles utilisés, nombre de passages renvoyés...)\n",
        "*   Comment adapter la solution en cas de base de données plus grande?\n",
        "*   Quels sont les avantages à utiliser une base de données vectorielle pour stocker les embeddings?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8YhkcgBwWpC5"
      }
    }
  ]
}